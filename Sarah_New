install.packages("freqparcoord")
library(freqparcoord)
data("prgeng", package = "freqparcoord")
help(prgeng)
#Check the data set
head(prgeng)     
str(prgeng)       
summary(prgeng)
######
help(prgeng, package = "freqparcoord")
data(prgeng)
# Examine the histogram of each variable to check the distribution and outliers,

#remove rows with missing values of wageinc, wkswrkd and agenote that in this project we want to inclued only employed people 
prgeng1 <- prgeng[prgeng$wageinc != 0, ]
prgeng2 <- prgeng1[prgeng1$wkswrkd != 0, ]
prgeng3 <- prgeng2[prgeng1$age != 0, ]
prgeng4 <- prgeng3[prgeng1$powspuma != 0, ]
prgeng5 <- prgeng4[prgeng1$age != 1, ]
prgeng5[!complete.cases(prgeng5), ]

# Remove rows with NA in specific column (e.g., wageinc)
cleaned_data <- prgeng5[!is.na(prgeng5$wageinc), ]
any(cleaned_data$age < 16)

head(cleaned_data)
summary(cleaned_data)
head(cleaned_data)
table(cleaned_data$engl)
######
table(cleaned_data$cit)
pairs(cleaned_data)
png("pairs_plot.png", width = 1200, height = 1200)
pairs(cleaned_data)
dev.off()
levels(cleaned_data$eng)
cleaned_data <- prgeng5[!is.na(prgeng5$wageinc), ]
#########

cleaned_data$eng <- as.factor(cleaned_data$eng)
levels(cleaned_data$eng)
##########
cleaned_data$cit <- as.factor(cleaned_data$cit)
levels(cleaned_data$cit)
#######
cleaned_data$educ <- as.factor(cleaned_data$educ)

levels(cleaned_data$educ)

#########
cleaned_data$occ <- as.factor(cleaned_data$occ)

levels(cleaned_data$occ)
##########
cleaned_data$birth <- as.factor(cleaned_data$birth)

levels(cleaned_data$birth)
############
cleaned_data$yrentry <- as.factor(cleaned_data$yrentry)

levels(cleaned_data$yrentry)
##############
cleaned_data$powspuma <- as.factor(cleaned_data$powspuma)

levels(cleaned_data$powspuma)
##################
plot(cleaned_data$wageinc~cleaned_data$ wkswrkd, data=cleaned_data, xlab="Number of weeks work",ylab="Annual Salary")
###########

#We can fit a regression line and show the fitted line on an extended range(BOOK):
MW<-lm(wageinc ~ wkswrkd, data=cleaned_data, na.action=na.exclude)
summary(MW)
##
plot(cleaned_data$wageinc~cleaned_data$ yrentry, data=cleaned_data, xlab="year of entry",ylab="Annual Salary")
YR<-lm(wageinc ~ yrentry, data=cleaned_data, na.action=na.exclude)
summary(YR)
##The variable is not statisticaly significant.
#There is strong statistically significant association between wage and #of weeks worked
##Examine the histogram of each variable to check the distribution and outliers,
# be on the look for potential data entry errors.
hist(cleaned_data$wageinc)
# There is outlier(extremly large)
##To make the graph clear, note that hist used for numerical data only
hist(cleaned_data$wageinc, breaks = 50, main = "Salary distribution", xlab = "Annual salary")
hist(cleaned_data$age, breaks = 50, main = "Age distribution", xlab = "Ages")#Normal with few oultier
hist(cleaned_data$wkswrkd, breaks = 50, main = "Weeks worked distribution", xlab = "Weeks worked")##Not normal at all
#############################################################
# Take an inital look at the complete regression model:
CompleteMODEL<-lm(wageinc ~ age+cit+educ+engl+occ+birth+wkswrkd+yrentry+powspuma+sex, data=cleaned_data, na.action=na.exclude)
summary(CompleteMODEL)
#cit and birth are not signifacant, also powspuma except for6020,6160,6180, 17500,19500(very strong),48170,48180(very strong)maybe those cause outliers.
##############################################################
ModelL<-lm(wageinc ~ birth, data=cleaned_data)
summary(ModelL)
######################
ModelC<-lm(wageinc ~ cit, data=cleaned_data)
summary(ModelC)
###What if I factored the variable Cit
cit.factor<-factor(cleaned_data$cit, levels=1:5,labels=c("Born in U.S","Born in Islands","Born Abroad","Naturalize","Not a citizen"))
ModelCF<-lm(wageinc ~ cit.factor, data=cleaned_data)
summary(ModelCF)

cleaned_data$cit <- as.factor(cleaned_data$cit)

cleaned_data$cit<- relevel(cleaned_data$cit, ref = "Born in U.S")


###################
ModelP<-lm(wageinc ~ powspuma, data=cleaned_data)
summary(ModelP)
##############################
ModelE<-lm(wageinc ~ educ, data=cleaned_data)
summary(ModelE)
##########################
ModelEng<-lm(wageinc ~ engl, data=cleaned_data)
summary(ModelEng)
########################
ModelWK<-lm(wageinc ~ wkswrkd, data=cleaned_data)
summary(ModelWK)
###############
ModelWK<-lm(wageinc ~ wkswrkd, data=cleaned_data)
summary(ModelWK)
#####################
NewModel<-lm(wageinc ~ age+educ+engl+occ+wkswrkd+yrentry+sex, data=cleaned_data)
summary(NewModel)
install.packages("car")
library(car)
vif(NewModel)## No concern about multicollinearity
residualPlots(NewModel, ~1, type="rstudent",
              id=list(labels=row.names(cleaned_data))) 
hist(residuals(NewModel))
qqPlot(NewModel)

##################
ModelS<-lm(wageinc ~ age+educ+engl+occ+wkswrkd+yrentry+sex+educ:wkswrkd, data=cleaned_data)
summary(ModelS)
vif(ModelS)
residualPlots(ModelS, ~1, type="rstudent",
              id=list(labels=row.names(cleaned_data))) 
hist(residuals(ModelS))
qqPlot(ModelS)

############################
# Note: Use "na.action = na.exclude" to avoid lurking errors in R caused by NAs in the data.
# If "na.action = na.exclude" is not specified, the default way lm() handles missing
# data will result in outputs from predict(), fitted(), residuals(), etc. having
# a different length than the original data -- all rows with missing data are removed.
# Setting "na.action=na.exclude" asks R to pad the outputs from predict(), fitted(), 
# residuals(), etc. with NA for those rows with missing data, so all the above 
# outputs have the right correspondence. 

MODEL<-lm(wageinc ~ age+educ+wkswrkd, data=cleaned_data, na.action=na.exclude)
summary(MODEL)
# Q-Q plot of the model
qqnorm(residuals(MODEL))
qqline(residuals(MODEL))#This is much better than mod , I think we must use this and not include categorical variables
plot(fitted(MODEL), rstudent(MODEL))
################################
# Transform Y with log(), fit the new model, and repeat the diagnostics
LOGMODEL<-lm(cleaned_data$log.wageinc~.,data=cleaned_data)
summary(LOGMODEL)
# Residual plots of the log-transformed model
plot(fitted(LOGMODEL), rstudent(LOGMODEL))
######Violation of Linearity Assumption 
### Impression: Most of the heteroskedasticity has NOT been fix! 
# We can plot the residuals against the individual Xs again. 
plot(cleaned_data$wageinc, residuals(LOGMODEL))
plot(cleaned_data$age, residuals(LOGMODEL))
plot(cleaned_data$educ, residuals(LOGMODEL))
plot(cleaned_data$wkswrkd, residuals(LOGMODEL))
plot(cleaned_data$engl, residuals(LOGMODEL))
plot(cleaned_data$occ, residuals(LOGMODEL))###delete this variable
plot(cleaned_data$sex, residuals(LOGMODEL))
plot(cleaned_data$cit, residuals(LOGMODEL))
plot(cleaned_data$powspuma, residuals(LOGMODEL))###
plot(cleaned_data$yrentry, residuals(LOGMODEL))##?Do we need to use polynomial
# Q-Q plot of the log-transformed model
qqnorm(residuals(LOGMODEL))
qqline(residuals(LOGMODEL))
plot(fitted(LOGMODEL), rstudent(LOGMODEL))
abline(h=0, col="red")
# Impression
##The log model is not good enough by qqnorm
# You can use the following code to identify the outlier
plot(fitted(LOGMODEL), rstudent(LOGMODEL), type='n')
text(fitted(LOGMODEL), rstudent(LOGMODEL), names(rstudent(LOGMODEL)))
rstudent(LOGMODEL)[21] # double check
### I will stop working on log model (codes below belongs to log)

#-------------------------------------------------------------------------------
# Extra: Box-cox transformation

# Use the following code to find out the optimal power lambda value

lambda <- powerTransform(MODEL)$lambda
lambda
##When I tried MODEL the matrix is not invertable(error)
#######
Square transformation
m22 <- lm(sqrt(wageinc) ~ age+educ+wkswrkd, data = cleaned_data)
# Residual plot square transformed model
plot(fitted(m22), rstudent(m22))
##############
# Q-Q plot of the square transformed model
qqnorm(residuals(m22))
qqline(residuals(m22))
##much better than log transformation

###Extra code if we need to use later
y<-prgeng$wageinc
x1<-prgeng$age
x2<-prgeng$cit
x3<-prgeng$educ
x4<-prgeng$engl
x5<-prgeng$occ
x6<-prgeng$birth
x7<-prgeng$wkswrkd
x8<-prgeng$yrentry
x9<-prgeng$powspuma
x10<-prgeng$gender
nonnative<-cleaned_data[cleaned_data$yrentry >0,]
##############################################################
##Example 1: Two categorical predictors + one numeric predictor

cleaned_data$sex.factor <- factor(cleaned_data$sex, levels = c(1, 2), labels = c("Male", "Female"))
cleaned_data$engl.factor <- factor(cleaned_data$engl, levels = c(0,10,11,20,21,30,31,40,41), labels = c("A", "B","C","D","E","F","G","H","I"))
cit.factor<-factor(cleaned_data$cit, levels=1:5,labels=c("Born in U.S","Born in Islands","Born Abroad","Naturalize","Not a citizen"))
table(cit.factor)
mt <- lm(cleaned_data$wageinc ~ cleaned_data$sex.factor + cleaned_data$engl.factor + cleaned_data$wkswrkd, data=cleaned_data)
summary(mt)
# Diagnostics
plot(predict(mt), rstudent(mt))
qqnorm(residuals(mt)); qqline(residuals(mt))

mt1 <- lm(cleaned_data$wageinc ~ cleaned_data$sex.factor + cleaned_data$engl.factor, data=cleaned_data)
summary(mt1)
plot(predict(mt1), rstudent(mt1))
qqnorm(residuals(mt1)); qqline(residuals(mt1))
# The F-test of general linear testing approach
anova(mt, mt1)
# Partial R-squared
anova(mt)
anova(mt1)
##############################################################
#Example:(Explanation in my note)
cleaned_data$sex.factor <- factor(cleaned_data$sex, levels = c(1, 2), labels = c("Male", "Female"))
m11<-lm(cleaned_data$wageinc ~ cleaned_data$sex.factor)
summary(m11)
# Multiple regression
table(cleaned_data$wkswrkd)# table(cleaned_data$wkswrkd)is not continous fine ???
ms1 <- lm(cleaned_data$wageinc ~ sex.factor + educ,data=cleaned_data)
summary(ms1)

ms2 <- lm(cleaned_data$wageinc ~ sex.factor + wkswrkd + sex.factor:wkswrkd,cleaned_data)
summary(ms2)
table(cleaned_data$wkswrkd)
ms3 <- lm(cleaned_data$wageinc ~ sex.factor + wkswrkd ,cleaned_data)
summary(ms3)

# Observed data scatterplot: Male data

 plot(cleaned_data$wkswrkd[cleaned_data$sex.factor == "Male"], cleaned_data$wageinc[cleaned_data$sex.factor == "Male"], 
       +      col="blue", pch=3,
       +      xlab="Number of weeks worked", ylab="Annual Salary", )

# Observed data scatterplot: Add Female data
 plot(
   cleaned_data$wkswrkd[cleaned_data$sex.factor == "Female"], 
   (nor worked)wageinc[cleaned_data$sex.factor == "Female"], 
   col = "orange", 
   pch = 5,
   xlab = "Number of weeks worked", 
   ylab = "Annual Salary"
 )
 

# Add the male regression line(not worked)

new.data <- data.frame(cleaned_data$sex.factor="Male", cleaned_data$wkswrkd = seq(from=min(cleaned_data$wkswrkd, na.rm=T), to=max(cleaned_data$wkswrkd, na.rm=T), length.out=30))
                       
my.pred <- predict(ms1, newdata=new.data)

lines(new.data$wkswrkd, my.pred, col="blue", pch=3, lwd=3)
help(data.frame)

# Add the female regression line

new.data <- data.frame(sex.factor = "female",cleaned_data$wkswrkd = seq(from=min(cleaned_data$wkswrkd, na.rm=T), to=max(cleaned_data$wkswrkd, na.rm=T), length.out=30))
                       

my.pred <- predict(ms3, newdata=new.data)

lines(new.data$bytests, my.pred, col="orange", pch=5, lwd=3)
###############################################################################################################
# Legend

legend("bottomright", legend=c("Male", "Female"), col=c("blue", "orange"), pch=c(3, 5))
#########Google help
# Step 1: Plot male data
plot(
  cleaned_data$wkswrkd[cleaned_data$sex.factor == "Male"], 
  cleaned_data$wageinc[cleaned_data$sex.factor == "Male"], 
  col = "blue", 
  pch = 3, 
  xlab = "Number of weeks worked", 
  ylab = "Annual Salary"
)

# Step 2: Add female data to the same plot
points(
  cleaned_data$wkswrkd[cleaned_data$sex.factor == "Female"], 
  cleaned_data$wageinc[cleaned_data$sex.factor == "Female"], 
  col = "orange", 
  pch = 5
)

# Step 3: Fit a linear model for male data and add the regression line
male_model <- lm(wageinc ~ wkswrkd, data = cleaned_data, subset = (sex.factor == "Male"))
abline(male_model, col = "blue", lwd = 2)  # Blue line for male regression

# Step 4: Fit a linear model for female data and add the regression line
female_model <- lm(wageinc ~ wkswrkd, data = cleaned_data, subset = (sex.factor == "Female"))
abline(female_model, col = "orange", lwd = 2)  # Orange line for female regression

# Step 5: Add a legend to distinguish the points and lines
legend(
  "topright", 
  legend = c("Male", "Female"), 
  col = c("blue", "orange"), 
  pch = c(3, 5), 
  lwd = 2
)


#############################################################################################################

##Example 2: Two categorical predictors + one numeric predictor
table(cleaned_data$educ)#10/24 class recording 2 time:1:13 and survey in cencus page 414

cleaned_data$educ.factor <- factor(cleaned_data$educ, levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16), labels = c("no schooling","4th grade","5th grade","7th grade","9th grade","10th grade","11th grade","12th grade","High school","less than one year college","one or two years college","associate degree","bachelors degree","masters","a professiona deal","doctorate")
table(factor(cleaned_data$educ.factor))
cleaned_data$engl.factor <- factor(cleaned_data$engl, levels = c(0,10,11,20,21,30,31,40,41), labels = c("A", "B","C","D","E","F","G","H","I"))
mt3 <- lm(cleaned_data$wageinc ~ cleaned_data$educ.factor + cleaned_data$engl.factor + cleaned_data$wkswrkd, data=cleaned_data)

summary(mt3)
# Diagnostics
plot(predict(mt), rstudent(mt))
qqnorm(residuals(mt)); qqline(residuals(mt))

mt2 <- lm(cleaned_data$wageinc ~ cleaned_data$sex.factor + cleaned_data$engl.factor, data=cleaned_data)
summary(mt2)
plot(predict(mt2), rstudent(mt2))
qqnorm(residuals(mt2)); qqline(residuals(mt2))
md <- lm(cleaned_data$wageinc ~ cleaned_data$sex.factor + cleaned_data$educ.factor, data=cleaned_data)
# Fit the linear model
mt1 <- lm(wageinc ~ sex.factor + engl.factor + educ.factor, data = cleaned_data)

# Check the model summary
summary(mt1)

# The F-test of general linear testing approach
anova(mt, mt1)
# Partial R-squared
anova(mt)
anova(mt1)
